{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Hyper Parameter Search on a remote cluster with Slurm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook consists of three parts:\n",
    "\n",
    "-Python file for single-process optimization with Optuna.\n",
    "  \n",
    "-Slurm file to launch the Python script in parallel on multiple processes.\n",
    "\n",
    "-Execution of the Slurm file on a remote cluster."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Write a python script to run the experiment "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to write a Python file (let's call it \"HP_search_parallel_cluster.py\" for example) that is similar to the first Optuna tutorial (without parallelization). It contains the necessary code to allow multiple trials to be run sequentially. Inside, we create our dataset, the objective function, create the study and run it.\n",
    "\n",
    "Next, we will create a Slurm file to launch \"HP_search_parallel_cluster.py\" multiple times. The parallelization will be achieved by running separate instances of the Python script simultaneously on multiple CPUs or nodes.\n",
    "\n",
    "Importantly, all trials' hyperparameters and results will be linked to the same storage, enabling efficient management and comparison of the optimization process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HP_search_parallel_cluster.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python file to run multiple trials sequentially :\n",
    "\n",
    "```python\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "import reservoirpy as rpy\n",
    "import optuna\n",
    "\n",
    "from reservoirpy.nodes import Reservoir, Ridge\n",
    "from reservoirpy.observables import nrmse, rsquare\n",
    "from reservoirpy.datasets import doublescroll\n",
    "\n",
    "from optuna.storages import JournalStorage, JournalFileStorage\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--nb_trials', type=int, required=True)\n",
    "parser.add_argument('--study_name', type=str, required=True)\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Data Preprocessing\n",
    "\n",
    "timesteps = 2000\n",
    "x0 = [0.37926545, 0.058339, -0.08167691]\n",
    "X = doublescroll(timesteps, x0=x0, method=\"RK23\")\n",
    "\n",
    "train_len = 1000\n",
    "\n",
    "X_train = X[:train_len]\n",
    "y_train = X[1 : train_len + 1]\n",
    "X_test = X[train_len : -1]\n",
    "y_test = X[train_len + 1:]\n",
    "\n",
    "dataset = ((X_train, y_train), (X_test, y_test))\n",
    "\n",
    "# Trial Fixed hyper-parameters\n",
    "\n",
    "nb_seeds = 3\n",
    "N = 500\n",
    "iss = 0.9\n",
    "ridge = 1e-7\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Record objective values for each trial\n",
    "    rpy.verbosity(0)\n",
    "    losses = []\n",
    "\n",
    "    # Trial generated parameters (with log scale)\n",
    "    sr = trial.suggest_float(\"sr_1\", 1e-2, 10, log=True)\n",
    "    lr = trial.suggest_float(\"lr_1\", 1e-3, 1, log=True)\n",
    "\n",
    "    for seed in range(nb_seeds):\n",
    "        reservoir = Reservoir(N,\n",
    "                              sr=sr,\n",
    "                              lr=lr,\n",
    "                              input_scaling=iss,\n",
    "                              seed=seed)\n",
    "        \n",
    "        readout = Ridge(ridge=ridge)\n",
    "        model = reservoir >> readout\n",
    "\n",
    "        # Train and test your model\n",
    "        predictions = model.fit(X_train, y_train).run(X_test)\n",
    "\n",
    "        # Compute the desired metric(s)\n",
    "        loss = nrmse(y_test, predictions, norm_value=np.ptp(X_train))\n",
    "\n",
    "        losses.append(loss)\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "# Define study parameters\n",
    "sampler = optuna.samplers.RandomSampler() \n",
    "log_name = f\"optuna-journal_{args.study_name}.log\"\n",
    "storage = JournalStorage(JournalFileStorage(log_name))\n",
    "\n",
    "# Create study\n",
    "study = optuna.create_study(\n",
    "    study_name=args.study_name,\n",
    "    direction=\"minimize\",\n",
    "    sampler=sampler,\n",
    "    storage=storage,\n",
    "    load_if_exists=True)\n",
    "\n",
    "\n",
    "# Launch the optimization for this specific job\n",
    "start = time.time()\n",
    "study.optimize(objective, n_trials=args.nb_trials)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Optimization done with {args.nb_trials} trials in {str(datetime.timedelta(seconds=end-start))}\")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : Write a slurm file "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To parallelize the \"HP_search_parallel_cluster.py\" Python file on a remote cluster , we will create multiple jobs to run it simultaneously with a slurm file (lets's call it \"HP_search_parallel_cluster.slurm\"). We can specify the number of jobs, their names, and the desired arguments for the Python file, including the number of trials per job (You can also run the Python file on multiple processes locally by using tools such as tmux).\n",
    "\n",
    "By launching x jobs with y trials_per_job, we enable efficient hyperparameter optimization across multiple CPUs or nodes on x*y trials in total, leveraging the cluster's resources effectively. \n",
    "\n",
    "Depending on the cluster you are working on, it might not be equipped with Slurm. In such cases, you'll need to utilize the job scheduling tool that is installed and available on that specific cluster, instead of Slurm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HP_search_parallel_cluster.slurm\n",
    "\n",
    "Slurm file to run the Python file on several processes simultaneously :\n",
    "\n",
    "```slurm\n",
    "#!/bin/bash\n",
    "\n",
    "#############################\n",
    "\n",
    "# Your job name (displayed by the queue)\n",
    "#SBATCH -J reservoirpy_parallel_HP_search_test\n",
    "\n",
    "# Specify the number of desired jobs in your job array (here 50)\n",
    "#SBATCH --array=0-49\n",
    "# Specify the maximum walltime per process (hh:mm::ss)\n",
    "#SBATCH -t 1:10:00\n",
    "\n",
    "# Specify the number of nodes(nodes=) and the number of cores per nodes(tasks-pernode=) to be used\n",
    "#SBATCH - N 1\n",
    "#SBATCH --ntasks-per-node=1\n",
    "\n",
    "# change working directory\n",
    "# SBATCH --chdir=.\n",
    "\n",
    "#############################\n",
    "\n",
    "# useful informations to print\n",
    "echo \"#############################\"\n",
    "echo \"User:\" $USER\n",
    "echo \"Date:\" `date`\n",
    "echo \"Host:\" `hostname`\n",
    "echo \"Directory:\" `pwd`\n",
    "echo \"SLURM_JOBID:\" $SLURM_JOBID\n",
    "echo \"SLURM_SUBMIT_DIR:\" $SLURM_SUBMIT_DIR\n",
    "echo \"SLURM_JOB_NODELIST:\" $SLURM_JOB_NODELIST\n",
    "echo \"#############################\"\n",
    "\n",
    "#############################\n",
    "\n",
    "# What you actually want to launch \n",
    "python3 HP_search_parallel_cluster.py --nb_trials 5 --study_name cluster_parallelization_tutorial\n",
    "# Total number of trials = nb_jobs * nb_trials = 50 * 5 = 250\n",
    "\n",
    "# all done\n",
    "echo \"Job finished\"\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Transfer the files to the cluster"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With \"HP_search_parallel_cluster.py\" and \"HP_search_parallel_cluster.slurm\" ready on our local directory, we can efficiently transfer them to a remote cluster if we use Linux with the `rsync` command. According to the cluster you use, it can be recommanded to use the [scp command](https://www.geeksforgeeks.org/scp-command-in-linux-with-examples/) instead.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bash commands to type in your local terminal, to transfer the Python and Slurm files to the cluster :\n",
    "\n",
    "```bash\n",
    "rsync -av HP_search_parallel_cluster.py username@cluster_adress\n",
    "rsync -av HP_search_parallel_cluster.slurm username@cluster_adress\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : Launch the slurm file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then from the cluster command line, you can launch the slurm file by using the \"sbatch\" command : "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slurm command to execute from the cluster command line, to launch the whole optimization process : \n",
    "\n",
    "```bash\n",
    "sbatch HP_search_parallel_cluster.slurm \n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After transferring the files to the remote cluster and launching the slurm file, we need to wait for the hyperparameter optimization to finish. The command line output of each job (and of the python file launched with this job) will be stored in files called \"slurm-<job_array_id>_<job_id>.out\". \n",
    "\n",
    "Here are some commands that can be useful:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bash commands to type directly in the cluster command line : \n",
    "\n",
    "```bash\n",
    "# Check the status of the jobs\n",
    "squeue -u $USER\n",
    "\n",
    "# Cancel the job_array (your can find the <job_array_id> with the precedent command line)\n",
    "scancel <job_array_id>\n",
    "\n",
    "# When the jobs are finished, check the content of the output files \n",
    "cat slurm-<job_array_id>_<job_id>.out\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 : Retrieve the optuna logs and visualize them"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the hyperparameter optimization is complete on the remote cluster, you can load the optuna storage back to your local directory using the `rsync` command. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bash command to type on your local machine to retrieve the optuna logs : \n",
    "\n",
    "```bash\n",
    "rsync -av username@cluster_adress/optuna-journal_cluster_parallelization_tutorial.log local_directory\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After transferring the log file, you can then load the study and plot the results in a jupyter notebook (using the same procedure as described in the first tutorial) : "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Load the study with the correct name and storage\n",
    "study = optuna.load_study(\n",
    "    study_name = f'{study_name}',\n",
    "    storage = storage\n",
    ")\n",
    "\n",
    "# Plot it with the function of your choice\n",
    "plot_slice(study)\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
